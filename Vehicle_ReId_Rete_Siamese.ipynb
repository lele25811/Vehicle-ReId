{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNNNuobXrvEqCTXJ6ceFd4J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lele25811/Vehicle-ReId/blob/main/Vehicle_ReId_Rete_Siamese.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vehicle Re-Id tramite Rete Siamese\n",
        "\n",
        "* dataset: vehicle-776\n",
        "* model: resnet50"
      ],
      "metadata": {
        "id": "RPvfCTzFAyYA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. Download datasets"
      ],
      "metadata": {
        "id": "23rXoovCBpBP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Monto il mio google drive (una volta montanto il contenuto di google drive sarà accessibile nella directory `/content/drive/My Drive`)\n",
        "from google.colab import drive\n",
        "!mkdir -p drive/\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cD5mM2KZBRdq",
        "outputId": "1c891e85-a5b4-4b8c-819b-f89e20dc86d4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Indoviduo il path del dataset\n",
        "dataset_path = \"/content/drive/MyDrive/datasets/VeRi.zip\""
      ],
      "metadata": {
        "id": "IboeQGI5BxqL"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creo la cartella `data`\n",
        "!mkdir -p data/"
      ],
      "metadata": {
        "id": "TdMb7-sKBzkW"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Copio il file nel Workspace\n",
        "!cp \"/content/drive/MyDrive/datasets/VeRi.zip\" /content/"
      ],
      "metadata": {
        "id": "A8MHBjeZB0t_"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Controllo il tipo di file scaricato\n",
        "!file VeRi.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4SVD1yy0B2J5",
        "outputId": "3fb0e424-7859-4234-f7ae-846f66f97276"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VeRi.zip: Zip archive data, at least v1.0 to extract, compression method=store\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Decomprimo il file zip\n",
        "!unzip -q /content/VeRi.zip -d data"
      ],
      "metadata": {
        "id": "uPUyWtBkB5EG"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Organizzazione Dataset\n",
        "1. Parsing del file XML\n",
        "\n",
        "Estraiamo le informazioni rilevanti dal file XML e le memorizziamo in un DataFrame Pandas\n",
        "\n",
        "    train_labels_df\n",
        "    test_labels_df\n",
        "\n",
        "che contengono:\n",
        "\n",
        "    nome immagine\n",
        "    Vehicle ID\n",
        "    Camera ID\n",
        "    Color ID\n",
        "    Type ID\n",
        "\n"
      ],
      "metadata": {
        "id": "__AAbOT3B6U8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xml.etree.ElementTree as ET\n",
        "import pandas as pd\n",
        "\n",
        "def parse_labels(label_path):\n",
        "    # Lista per memorizzare i dati\n",
        "    data = []\n",
        "\n",
        "    # Aprire il file con la codifica esplicita\n",
        "    with open(label_path, \"r\", encoding=\"ascii\", errors=\"ignore\") as f:\n",
        "      content = f.read() # Leggi il contenuto ignorando gli errori\n",
        "\n",
        "      try:\n",
        "        tree = ET.ElementTree(ET.fromstring(content))  # Parso direttamente il contenuto\n",
        "        root = tree.getroot()  # Ottieni il nodo radice\n",
        "        # Iterare attraverso i nodi Item\n",
        "        for item in root.find('Items').findall(\"Item\"):\n",
        "          image_name = item.get(\"imageName\")\n",
        "          vehicle_id = item.get(\"vehicleID\")\n",
        "          camera_id = item.get(\"cameraID\")\n",
        "          color_id = item.get(\"colorID\")\n",
        "          type_id = item.get(\"typeID\")\n",
        "\n",
        "          # Aggiungere i dati alla lista\n",
        "          data.append({\n",
        "            \"image_name\": image_name,\n",
        "            \"vehicle_id\": int(vehicle_id),\n",
        "            \"camera_id\": camera_id,\n",
        "            \"color_id\": int(color_id),\n",
        "            \"type_id\": int(type_id)\n",
        "          })\n",
        "      except ET.ParseError as e:\n",
        "        print(f\"Errore di parsing: {e}\")\n",
        "        return {}\n",
        "\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "# Percorso al file XML\n",
        "train_label_path = \"/content/data/VeRi/train_label.xml\"\n",
        "test_label_path = \"/content/data/VeRi/test_label.xml\"\n",
        "\n",
        "# Creo un dataframe per le immagini di train e test\n",
        "train_labels_df = parse_labels(train_label_path)\n",
        "test_labels_df = parse_labels(test_label_path)\n",
        "\n",
        "# Recupero le dimensioni dei dataset (righe e colonne)\n",
        "train_df_rows, train_df_cols = train_labels_df.shape\n",
        "test_df_rows, test_df_cols = test_labels_df.shape\n",
        "\n",
        "print(f\"train dataframe -> rows: {train_df_rows}, cols: {train_df_cols}\")\n",
        "print(f\"test dataframe -> rows: {test_df_rows}, cols: {test_df_cols}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yhvIEYcECI5i",
        "outputId": "c90c7c2e-34d5-4ba7-b49a-9bdcd86c61c6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train dataframe -> rows: 37746, cols: 5\n",
            "test dataframe -> rows: 11579, cols: 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Controlliamo i primi elementi per confermare che le colonne siano riempite correttamente\n",
        "print(train_labels_df.head())\n",
        "print(test_labels_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ua_K5Vh_CbRx",
        "outputId": "6cc5c6d6-1d80-470e-aaa6-0177eac1563e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                 image_name  vehicle_id camera_id  color_id  type_id\n",
            "0  0001_c001_00016450_0.jpg           1      c001         1        4\n",
            "1  0001_c001_00016460_0.jpg           1      c001         1        4\n",
            "2  0001_c001_00016470_0.jpg           1      c001         1        4\n",
            "3  0001_c001_00016480_0.jpg           1      c001         1        4\n",
            "4  0001_c001_00016490_0.jpg           1      c001         1        4\n",
            "                 image_name  vehicle_id camera_id  color_id  type_id\n",
            "0  0002_c002_00030600_0.jpg           2      c002         1        4\n",
            "1  0002_c002_00030605_1.jpg           2      c002         1        4\n",
            "2  0002_c002_00030615_1.jpg           2      c002         1        4\n",
            "3  0002_c002_00030625_1.jpg           2      c002         1        4\n",
            "4  0002_c002_00030640_0.jpg           2      c002         1        4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Controlliamo che non ci siano valori nulli (stampa quanti NULL ci sono)\n",
        "print(train_labels_df.isnull().sum())\n",
        "print(test_labels_df.isnull().sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HET9KAmZCdLh",
        "outputId": "9b28eeae-21ad-42fc-e5f3-f4b2641e7b6f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "image_name    0\n",
            "vehicle_id    0\n",
            "camera_id     0\n",
            "color_id      0\n",
            "type_id       0\n",
            "dtype: int64\n",
            "image_name    0\n",
            "vehicle_id    0\n",
            "camera_id     0\n",
            "color_id      0\n",
            "type_id       0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Color ID unici:\", train_labels_df[\"color_id\"].nunique())\n",
        "print(\"Type ID unici:\", train_labels_df[\"type_id\"].nunique())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JpmBrhzpCemR",
        "outputId": "000af225-4f59-4746-da27-7a7b51f1bb2f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Color ID unici: 10\n",
            "Type ID unici: 9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Creazione delle coppie per la rete siamese\n",
        "Devo creare coppie di immagini (chiamate positive e negative) per la rene siamese:\n",
        "* Coppie positive: Immagini dello stesso veicolo (stesso id_veicolo).\n",
        "* Coppie negative: Immagini di veicoli diversi (differenti id_veicolo).\n",
        "\n"
      ],
      "metadata": {
        "id": "Bs9miN6cHOZX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Descrizione funzionamento:\n",
        "* Coppie Positive:\n",
        "\n",
        "    Per ogni vehicle_id, raccogli tutte le immagini associate.\n",
        "    Genera combinazioni uniche di immagini appartenenti allo stesso veicolo (images_same_vehicle).\n",
        "\n",
        "* Coppie Negative:\n",
        "\n",
        "    Per ogni immagine, seleziona casualmente un'altra immagine da un veicolo diverso (negative_vehicle_id).\n",
        "    Ripeti questo processo per un numero configurabile di coppie negative (controllato da num_negative_pairs).\n",
        "\n",
        "* Output:\n",
        "\n",
        "    Ogni coppia è una tupla del tipo (img1, img2, label), dove label è 1 per coppie positive e 0 per negative."
      ],
      "metadata": {
        "id": "_Zl7WreUInYU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "def create_pairs_from_dataframe(dataframe, num_negative_pairs=1):\n",
        "    \"\"\"\n",
        "    Genera coppie positive e negative da un dataframe di immagini.\n",
        "\n",
        "    Args:\n",
        "        dataframe (pd.DataFrame): Contiene colonne ['nome_image', 'vehicle_id', 'camera_id', 'color_id', 'type_id'].\n",
        "        num_negative_pairs (int): Numero di coppie negative da creare per ogni coppia positiva.\n",
        "\n",
        "    Returns:\n",
        "        List[Tuple]: Lista di tuple (img1, img2, label), dove label è 1 per coppie positive e 0 per negative.\n",
        "    \"\"\"\n",
        "    # Organizza i dati per vehicle_id\n",
        "    data_by_id = dataframe.groupby('vehicle_id')\n",
        "    unique_ids = dataframe['vehicle_id'].unique()\n",
        "\n",
        "    pairs = []\n",
        "\n",
        "    for vehicle_id in unique_ids:\n",
        "        # Estrai tutte le immagini per lo stesso vehicle_id\n",
        "        images_same_vehicle = data_by_id.get_group(vehicle_id)['image_name'].values\n",
        "\n",
        "        # Genera coppie positive\n",
        "        for i in range(len(images_same_vehicle)):\n",
        "            for j in range(i + 1, len(images_same_vehicle)):\n",
        "                pairs.append((images_same_vehicle[i], images_same_vehicle[j], 1))  # Label = 1 (positive)\n",
        "\n",
        "        # Genera coppie negative\n",
        "        other_ids = [vid for vid in unique_ids if vid != vehicle_id]\n",
        "        for _ in range(num_negative_pairs * len(images_same_vehicle)):\n",
        "            # Scegli un'immagine casuale dallo stesso vehicle_id\n",
        "            img1 = random.choice(images_same_vehicle)\n",
        "            # Scegli un'immagine casuale da un altro vehicle_id\n",
        "            negative_vehicle_id = random.choice(other_ids)\n",
        "            img2 = data_by_id.get_group(negative_vehicle_id)['image_name'].sample().iloc[0]\n",
        "            pairs.append((img1, img2, 0))  # Label = 0 (negative)\n",
        "\n",
        "    return pairs\n",
        "\n",
        "# Esempio di utilizzo con train_labels_df\n",
        "train_pairs = create_pairs_from_dataframe(train_labels_df, num_negative_pairs=1)\n",
        "test_pairs = create_pairs_from_dataframe(test_labels_df, num_negative_pairs=1)\n",
        "\n",
        "# Stampa alcune coppie generate\n",
        "print(\"Train pairs example:\", train_pairs[:5])\n",
        "print(\"Test pairs example:\", test_pairs[:5])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WRoYEopEIVD0",
        "outputId": "2ca72f83-2a45-4fcd-e905-1f73f1a625ea"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train pairs example: [('0001_c001_00016450_0.jpg', '0001_c001_00016460_0.jpg', 1), ('0001_c001_00016450_0.jpg', '0001_c001_00016470_0.jpg', 1), ('0001_c001_00016450_0.jpg', '0001_c001_00016480_0.jpg', 1), ('0001_c001_00016450_0.jpg', '0001_c001_00016490_0.jpg', 1), ('0001_c001_00016450_0.jpg', '0001_c001_00016500_0.jpg', 1)]\n",
            "Test pairs example: [('0002_c002_00030600_0.jpg', '0002_c002_00030605_1.jpg', 1), ('0002_c002_00030600_0.jpg', '0002_c002_00030615_1.jpg', 1), ('0002_c002_00030600_0.jpg', '0002_c002_00030625_1.jpg', 1), ('0002_c002_00030600_0.jpg', '0002_c002_00030640_0.jpg', 1), ('0002_c002_00030600_0.jpg', '0002_c002_00030670_0.jpg', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "import os"
      ],
      "metadata": {
        "id": "CZVaBWETJCQV"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class SiamesePairDataset(Dataset):\n",
        "    def __init__(self, pairs, image_dir, transform=None):\n",
        "        \"\"\"\n",
        "        Dataset per coppie siamese.\n",
        "\n",
        "        Args:\n",
        "            pairs (list): Lista di tuple (img1, img2, label).\n",
        "            image_dir (str): Directory delle immagini.\n",
        "            transform (callable, optional): Trasformazioni per le immagini.\n",
        "        \"\"\"\n",
        "        self.pairs = pairs\n",
        "        self.image_dir = image_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img1_name, img2_name, label = self.pairs[idx]\n",
        "        img1_path = os.path.join(self.image_dir, img1_name)\n",
        "        img2_path = os.path.join(self.image_dir, img2_name)\n",
        "\n",
        "        img1 = Image.open(img1_path).convert('RGB')\n",
        "        img2 = Image.open(img2_path).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            img1 = self.transform(img1)\n",
        "            img2 = self.transform(img2)\n",
        "\n",
        "        return img1, img2, torch.tensor(label, dtype=torch.float32)"
      ],
      "metadata": {
        "id": "5fXJT6WwI63B"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "# Trasformazioni per l'allenamento e il test\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((64, 64)),  # ResNet richiede input 224x224\n",
        "    transforms.ToTensor(),  # Converte l'immagine in tensore\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),  # Normalizzazione ImageNet\n",
        "])"
      ],
      "metadata": {
        "id": "x2iv5th5JaE3"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Configura il DataLoader\n",
        "train_dataset = SiamesePairDataset(train_pairs, image_dir='/content/data/VeRi/image_train', transform=transform)\n",
        "test_dataset = SiamesePairDataset(test_pairs, image_dir='/content/data/VeRi/image_test', transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False)\n",
        "\n",
        "# Esegui il training con il modello e la funzione di perdita definiti precedentemente."
      ],
      "metadata": {
        "id": "d_UPoRx9JKQt"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modello con resnet50"
      ],
      "metadata": {
        "id": "wbwurfjHMtf9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "\n",
        "class SiameseNetworkresnet50(nn.Module):\n",
        "    def __init__(self, embedding_dim=128):\n",
        "        \"\"\"\n",
        "        Modello Siamese basato su ResNet50 come backbone.\n",
        "\n",
        "        Args:\n",
        "            embedding_dim (int): Dimensione del vettore di embedding.\n",
        "        \"\"\"\n",
        "        super(SiameseNetwork, self).__init__()\n",
        "\n",
        "        # ResNet50 pre-addestrata come backbone\n",
        "        base_model = models.resnet50(pretrained=True)\n",
        "        self.feature_extractor = nn.Sequential(*list(base_model.children())[:-1])  # Rimuovi il layer fully connected\n",
        "\n",
        "        # Layer fully connected per ridurre la dimensione dell'embedding\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(base_model.fc.in_features, embedding_dim),  # Riduzione alla dimensione specificata\n",
        "            nn.ReLU(),  # Attivazione\n",
        "            nn.Linear(embedding_dim, embedding_dim)  # Proiezione finale\n",
        "        )\n",
        "\n",
        "    def forward_one(self, x):\n",
        "        \"\"\"\n",
        "        Passa un'immagine attraverso il modello per ottenere un embedding.\n",
        "\n",
        "        Args:\n",
        "            x (Tensor): Immagine di input.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Vettore di embedding.\n",
        "        \"\"\"\n",
        "        x = self.feature_extractor(x)  # Estrai le caratteristiche\n",
        "        x = x.view(x.size(0), -1)  # Appiattisci le caratteristiche (global pooling)\n",
        "        x = self.fc(x)  # Applica il layer fully connected\n",
        "        return x\n",
        "\n",
        "    def forward(self, img1, img2):\n",
        "        \"\"\"\n",
        "        Passa una coppia di immagini attraverso il modello.\n",
        "\n",
        "        Args:\n",
        "            img1 (Tensor): Prima immagine.\n",
        "            img2 (Tensor): Seconda immagine.\n",
        "\n",
        "        Returns:\n",
        "            Tuple[Tensor, Tensor]: Embedding delle due immagini.\n",
        "        \"\"\"\n",
        "        emb1 = self.forward_one(img1)  # Ottieni embedding per la prima immagine\n",
        "        emb2 = self.forward_one(img2)  # Ottieni embedding per la seconda immagine\n",
        "        return emb1, emb2\n"
      ],
      "metadata": {
        "id": "BBsAzORqKOm9"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class LightSiameseNetwork(nn.Module):\n",
        "    def __init__(self, embedding_dim=64):\n",
        "        super(LightSiameseNetwork, self).__init__()\n",
        "\n",
        "        # Feature extractor semplice e leggero\n",
        "        self.feature_extractor = nn.Sequential(\n",
        "            nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1),  # (C=16, H, W)\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),  # (C=16, H/2, W/2)\n",
        "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),  # (C=32, H/2, W/2)\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),  # (C=32, H/4, W/4)\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),  # (C=64, H/4, W/4)\n",
        "            nn.ReLU(),\n",
        "            nn.AdaptiveAvgPool2d(1),  # Output: (64, 1, 1)\n",
        "            nn.Flatten()  # Output: (batch_size, 64)\n",
        "        )\n",
        "\n",
        "        # Proiezione nello spazio degli embedding\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(64, embedding_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(embedding_dim, embedding_dim)\n",
        "        )\n",
        "\n",
        "    def forward_once(self, x):\n",
        "        \"\"\"\n",
        "        Esegue una forward pass per una singola immagine.\n",
        "        \"\"\"\n",
        "        x = self.feature_extractor(x)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "    def forward(self, input1, input2):\n",
        "        \"\"\"\n",
        "        Forward pass per due immagini.\n",
        "        \"\"\"\n",
        "        output1 = self.forward_once(input1)\n",
        "        output2 = self.forward_once(input2)\n",
        "        return output1, output2\n"
      ],
      "metadata": {
        "id": "M4PNlhMRM123"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ContrastiveLoss(nn.Module):\n",
        "    def __init__(self, margin=1.0):\n",
        "        \"\"\"\n",
        "        Inizializza la Contrastive Loss con il margine dato.\n",
        "\n",
        "        Args:\n",
        "            margin (float): Margine per coppie negative.\n",
        "        \"\"\"\n",
        "        super(ContrastiveLoss, self).__init__()\n",
        "        self.margin = margin\n",
        "\n",
        "    def forward(self, emb1, emb2, label):\n",
        "        \"\"\"\n",
        "        Calcola la Contrastive Loss.\n",
        "\n",
        "        Args:\n",
        "            emb1 (Tensor): Embedding della prima immagine (batch_size, embedding_dim).\n",
        "            emb2 (Tensor): Embedding della seconda immagine (batch_size, embedding_dim).\n",
        "            label (Tensor): Label binaria (1 per positivo, 0 per negativo).\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Valore della loss.\n",
        "        \"\"\"\n",
        "        # Distanza euclidea tra gli embedding\n",
        "        distance = F.pairwise_distance(emb1, emb2)\n",
        "\n",
        "        # Loss per coppie positive (label=1) e negative (label=0)\n",
        "        positive_loss = label * (distance ** 2)\n",
        "        negative_loss = (1 - label) * F.relu(self.margin - distance) ** 2\n",
        "\n",
        "        # Loss totale (media del batch)\n",
        "        loss = 0.5 * (positive_loss + negative_loss).mean()\n",
        "        return loss\n"
      ],
      "metadata": {
        "id": "O5uBnVyGKoyi"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm  # Importa tqdm per il monitoraggio\n",
        "\n",
        "# Configurazione del dispositivo\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Utilizzando dispositivo: {device}\")\n",
        "\n",
        "# Modello, perdita e ottimizzatore\n",
        "model = LightSiameseNetwork(embedding_dim=64).to(device)\n",
        "criterion = ContrastiveLoss(margin=1.0)  # Margin per la Contrastive Loss\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "# Loop di allenamento\n",
        "num_epochs = 1\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    # Itera sul train_loader con tqdm\n",
        "    train_loader_tqdm = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\")\n",
        "    for img1, img2, label in train_loader_tqdm:\n",
        "        img1, img2, label = img1.to(device), img2.to(device), label.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        emb1, emb2 = model(img1, img2)\n",
        "        loss = criterion(emb1, emb2, label)\n",
        "\n",
        "        # Backward pass e aggiornamento\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        # Aggiorna la descrizione del progresso\n",
        "        train_loader_tqdm.set_postfix(loss=loss.item())\n",
        "\n",
        "    avg_loss = running_loss / len(train_loader)\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    # Valutazione ogni epoca\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        # Itera sul test_loader con tqdm\n",
        "        test_loader_tqdm = tqdm(test_loader, desc=f\"Valutazione Epoch {epoch+1}\", unit=\"batch\")\n",
        "        for img1, img2, label in test_loader_tqdm:\n",
        "            img1, img2 = img1.to(device), img2.to(device)\n",
        "            label = label.to(device)\n",
        "\n",
        "            # Embedding e predizione\n",
        "            emb1, emb2 = model(img1, img2)\n",
        "            distance = torch.nn.functional.pairwise_distance(emb1, emb2)\n",
        "            predictions = (distance < 0.5).float()  # Soglia di 0.5 per predire \"stesso veicolo\"\n",
        "            correct += (predictions == label).sum().item()\n",
        "            total += label.size(0)\n",
        "\n",
        "    accuracy = correct / total\n",
        "    print(f\"Accuracy dopo epoch {epoch+1}: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kr_p1NUrJxp6",
        "outputId": "10564289-afaa-425a-a0c5-48bce60b46b5"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Utilizzando dispositivo: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1: 100%|██████████| 6816/6816 [1:34:06<00:00,  1.21batch/s, loss=0.00408]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/1], Loss: 0.0092\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valutazione Epoch 1: 100%|██████████| 1792/1792 [23:31<00:00,  1.27batch/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy dopo epoch 1: 0.9769\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}